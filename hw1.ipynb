{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import jieba\n",
    "import collections\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log10\n",
    "\n",
    "jieba.set_dictionary('./dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(articles):\n",
    "    dataset = list()\n",
    "    for article in articles[:10000]: # 先少一點\n",
    "        sentences = article[:-1].split('\\t') # sentences: list\n",
    "        for i in range(len(sentences)): # iterate 時不會直接改到原本容器, 所以要用 index traverse\n",
    "            sentences[i] = re.sub(r'\\W+', \"\", sentences[i])\n",
    "        parsed_sentences = list()\n",
    "        for sentence in sentences:\n",
    "            words = list(jieba.cut(sentence, cut_all=False)) # 一句一句切\n",
    "            parsed_sentences.append(words) # 切過的句子組合成文章\n",
    "        # print(parsed_sentences)\n",
    "        dataset.append(parsed_sentences) # 文章組合成檔案集 # parsed_sentences = article\n",
    "    # print(dataset)\n",
    "    return list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(article): # t: 詞\n",
    "    word_statistics_in_article = collections.Counter()\n",
    "    for sentence in article:\n",
    "        word_statistics_in_article += collections.Counter(sentence)\n",
    "    # print(word_statistics_in_article)\n",
    "    ### 計算 tf\n",
    "    total = sum(word_statistics_in_article.values(), 0.0)\n",
    "    word_tf_in_article = {key: round((val/total), 2) for key, val in sorted(word_statistics_in_article.items(), key=lambda x: x[1], reverse=True)}\n",
    "    # print(pd.DataFrame(list(word_tf_in_article.items()), columns=['word', 'tf'])) # DataFrame 好看但我不太會用\n",
    "    return word_tf_in_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(tf_per_article_table, q): # terms: list of all words\n",
    "    word_appear_count = collections.Counter()\n",
    "    for i in range(len(tf_per_article_table)):\n",
    "        word_appear_count += collections.Counter({key: 1 for key in tf_per_article_table[i].keys()})\n",
    "    \n",
    "    idf_table = {word: round(log10(q / count), 3) for word, count in word_appear_count.items()}\n",
    "    return idf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1), ('B', 1)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'A': 1, 'B': 1}\n",
    "b = {'B': 1, 'C': 1}\n",
    "# collections.Counter(a) + collections.Counter(b)\n",
    "list(a.items())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tf_in_dataset(dataset):\n",
    "    all_words_count = collections.Counter()\n",
    "    for article in dataset:\n",
    "        for sentence in article:\n",
    "            all_words_count += collections.Counter(sentence)\n",
    "    total = sum(all_words_count.values(), 0.0)\n",
    "    all_words_tf = {key: round((val/total), 3) for key, val in sorted(all_words_count.items(), key=lambda x: x[1], reverse=True)}\n",
    "    return all_words_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(tf_t_d, idf_t_D):\n",
    "    ### To Do\n",
    "    # print(idf_t_D)\n",
    "    table = dict()\n",
    "    for t, tf in tf_t_d.items():\n",
    "        # print(t, tf, idf_t_D[t])\n",
    "        table[t] = round((tf * idf_t_D[t]), 3)\n",
    "    # print(table)\n",
    "    return {key: val for key, val in sorted(table.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_100(tf_idf_table):\n",
    "    skip_flag = {key: False for key in tf_idf_table.keys()}\n",
    "    rank_100 = {'a': 0.0}\n",
    "    min_in_rank = ('a', 0.0)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        stop_flag = True\n",
    "        for i in range(len(tf_idf_table)):\n",
    "            if idx >= len(tf_idf_table[i].items()): # 第 i 篇文章已經掃完了, 直接往下一篇\n",
    "                continue\n",
    "            if skip_flag[i] != True: # 第 i 篇文章還沒掃完, 且不 skip\n",
    "                word, weight = tuple(tf_idf_table[i].items())[idx] # list(tf_idf_table[i].items())[idx] 第 i 個文章中的 第 idx 大的\n",
    "                if weight > min_in_rank[1]:  # if 文章 i 第一高權重的值 > rank 100 中最小值\n",
    "                    rank_100[ word ] = weight # 加進新血\n",
    "                    if len(rank_100) > 100: # 若列表超過 100, 該踢人 (ps: 還沒滿的話不排也沒差)\n",
    "                        rank_100.pop(min_in_rank[0]) # 殺掉最小\n",
    "                        new_rank_list = sorted(rank_100.items(), key=lambda x: x[1], reverse=True) # 重新排序後的列表\n",
    "                        min_in_rank = new_rank_list[-1] # 更新最小\n",
    "                        rank_100 = {key: val for key, val in new_rank_list} # 用重排後的 list 更新 rank_100\n",
    "                    \n",
    "                    stop_flag = False\n",
    "                else:\n",
    "                    skip_flag[i] = True\n",
    "                \n",
    "        if stop_flag == True: break # 都沒更新 === skip_flag 全部變 True 了, 跳出迴圈\n",
    "        idx += 1\n",
    "    print(rank_100, len(rank_100))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'嘿嘿嘿': 2.885, '面議': 2.68, '廠廠': 2.642, '歐拉': 2.626, '霍霍': 2.28, '蛤': 2.212, '車手': 2.0, '之於': 2.0, '450000': 2.0, '啵': 2.0, 'Sowhatstheplan': 2.0, 'CARHAND': 2.0, 'Iwanttogonu': 2.0, '大嘴': 1.88, '金正恩': 1.849, '嫑': 1.699, '喔喔': 1.611, '曰': 1.56, '欸': 1.524, '一棟': 1.339, '憂傷': 1.32, '希拉蕊': 1.32, '顏楷': 1.32, 'edsheeran': 1.32, '鳳飛飛': 1.32, 'push': 1.32, 'MakeAmericaGreatAgain': 1.32, 'Thatheartiscold': 1.32, '林依晨': 1.221, '胖胖': 1.221, '蛋黃': 1.221, '準備好': 1.218, 'づ': 1.2, '姆咪': 1.198, '哈哈': 1.183, '孔雀': 1.163, '雷電': 1.16, 'Pavone': 1.16, 'agguy': 1.16, '色狼': 1.16, '吃吃': 1.121, '歐歐': 1.12, '皇民化': 1.08, '提琴': 1.08, '盡頭': 1.073, '恭喜': 1.063, '白冰冰': 1.063, '日': 1.061, '餓': 1.043, '新歌': 1.041, '黎明': 1.019, '22k': 1.005, 'Snowbaby': 1.0, '咕咕': 1.0, '888': 1.0, 'Ivebecomesonumb': 1.0, 'Letitbe': 1.0, '無情': 1.0, '隔宿': 1.0, '下注': 1.0, '帕': 1.0, '小熊': 1.0, 'KDKB': 1.0, '睡不著覺': 1.0, '蛤喇': 1.0, 'Ｔ': 1.0, '貌': 1.0, 'GetWild': 1.0, '壞運': 1.0, '親友團': 1.0, 'LaLaLaLaLaLoveSong': 1.0, '班馬班馬': 1.0, 'q': 1.0, '射鵰': 1.0, 'linkinprok': 1.0, '甲殼蟲': 1.0, 'HENTAI': 1.0, '一匹': 1.0, '野馬': 1.0, '對不起': 0.999, '豬': 0.999, '魚': 0.99, '肥皂': 0.985, '加油': 0.964, '金剛': 0.934, '難吃': 0.929, '蛞蝓': 0.925, '元宵': 0.925, '好學': 0.925, '芬蘭': 0.925, '洗乾淨': 0.925, '田馥甄': 0.925, 'googleplay': 0.925, 'KD': 0.925, '自自': 0.925, '小講': 0.925, '樂隊': 0.925, '課長': 0.925, '冉冉': 0.925, 'GAY': 0.925} 100\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file = open(\"./hw1-dataset.txt\", mode='r')\n",
    "    articles = file.readlines()\n",
    "    dataset = preprocessing(articles)\n",
    "    \n",
    "    ### 統計高頻\n",
    "    tf_whole_dataset_table = word_tf_in_dataset(dataset) # 高頻詞(已排序)\n",
    "            \n",
    "    ### 建 tf table\n",
    "    tf_per_article_table = dict()\n",
    "    for article_no in range(len(dataset)):\n",
    "        tf_per_article_table[article_no] = tf(dataset[article_no])\n",
    "    # print(tf_per_article_table) # 取值: tf_per_article_table[article_no][t] = tf(t, d)\n",
    "    \n",
    "    ### 建 idf table\n",
    "    idf_table = idf(tf_per_article_table, len(dataset))\n",
    "    # print(idf_table)\n",
    "    \n",
    "    ### 統計 tf-idf 高\n",
    "    tf_idf_table = dict()\n",
    "    for article_no in range(len(dataset)):\n",
    "        tf_idf_table[article_no] = tf_idf(tf_per_article_table[article_no], idf_table)\n",
    "    # print(tf_idf_table)\n",
    "    rank_100(tf_idf_table)\n",
    "    \n",
    "    file.close()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['為什麼 聖結石 會被酸而 這群人 不會？\\t質感 劇本 成員 都差很多好嗎 不要拿腎結石來污辱這群人\\n', '為什麼慶祝228會被罵可是慶端午不會？\\t因為屈原不是台灣人，是楚國人。\\n', '有沒有戰神阿瑞斯的八卦?\\t爵士就是阿瑞斯 男主角最後死了\\n']\n"
     ]
    }
   ],
   "source": [
    "dataset = open(\"./hw1-dataset.txt\", mode='r')\n",
    "articles = list()\n",
    "for article in dataset: # article: 一行算一個文章\n",
    "    articles.append(article)\n",
    "    \n",
    "dataset.close()\n",
    "print(articles[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
