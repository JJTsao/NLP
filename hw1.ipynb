{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import jieba\n",
    "import collections\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log10\n",
    "\n",
    "jieba.set_dictionary('./dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(articles):\n",
    "    dataset = list()\n",
    "    for article in articles[:10]: # 先少一點\n",
    "        sentences = article[:-1].split('\\t') # sentences: list\n",
    "        for i in range(len(sentences)): # iterate 時不會直接改到原本容器, 所以要用 index traverse\n",
    "            sentences[i] = re.sub(r'\\W+', \"\", sentences[i])\n",
    "        parsed_sentences = list()\n",
    "        for sentence in sentences:\n",
    "            words = list(jieba.cut(sentence, cut_all=False)) # 一句一句切\n",
    "            parsed_sentences.append(words) # 切過的句子組合成文章\n",
    "        # print(parsed_sentences)\n",
    "        dataset.append(parsed_sentences) # 文章組合成檔案集 # parsed_sentences = article\n",
    "    # print(dataset)\n",
    "    return list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(article): # t: 詞\n",
    "    word_statistics_in_article = collections.Counter()\n",
    "    for sentence in article:\n",
    "        word_statistics_in_article += collections.Counter(sentence)\n",
    "    # print(word_statistics_in_article)\n",
    "    ### 計算 tf\n",
    "    total = sum(word_statistics_in_article.values(), 0.0)\n",
    "    word_tf_in_article = {key: round((val/total), 2) for key, val in sorted(word_statistics_in_article.items(), key=lambda x: x[1], reverse=True)}\n",
    "    # print(pd.DataFrame(list(word_tf_in_article.items()), columns=['word', 'tf'])) # DataFrame 好看但我不太會用\n",
    "    return word_tf_in_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(tf_per_article_table, q): # terms: list of all words\n",
    "    word_appear_count = collections.Counter()\n",
    "    for i in range(len(tf_per_article_table)):\n",
    "        word_appear_count += collections.Counter({key: 1 for key in tf_per_article_table[i].keys()})\n",
    "    \n",
    "    idf_table = {word: round(log10(q / count), 3) for word, count in word_appear_count.items()}\n",
    "    return idf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 1, 'B': 2, 'C': 1})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'A': 1, 'B': 1}\n",
    "b = {'B': 1, 'C': 1}\n",
    "collections.Counter(a) + collections.Counter(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tf_in_dataset(dataset):\n",
    "    all_words_count = collections.Counter()\n",
    "    for article in dataset:\n",
    "        for sentence in article:\n",
    "            all_words_count += collections.Counter(sentence)\n",
    "    total = sum(all_words_count.values(), 0.0)\n",
    "    all_words_tf = {key: round((val/total), 3) for key, val in sorted(all_words_count.items(), key=lambda x: x[1], reverse=True)}\n",
    "    return all_words_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(tf_t_d, idf_t_D):\n",
    "    ### To Do\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'這群': 1.0, '人': 0.523, '為什麼': 0.398, '聖': 1.0, '結石': 1.0, '會': 0.523, '被': 0.699, '酸': 1.0, '而': 1.0, '不會': 0.699, '質感': 1.0, '劇本': 1.0, '成員': 1.0, '都': 0.699, '差': 1.0, '很多': 1.0, '好': 1.0, '嗎': 0.699, '不要': 1.0, '拿': 1.0, '腎結石': 1.0, '來': 0.699, '污辱': 1.0, '是': 0.699, '慶祝': 1.0, '228': 1.0, '罵': 1.0, '可是': 1.0, '慶': 1.0, '端午': 1.0, '因為': 1.0, '屈原': 1.0, '不': 0.699, '台灣人': 1.0, '楚': 1.0, '國人': 1.0, '阿瑞': 1.0, '斯': 1.0, '有沒有': 1.0, '戰神': 1.0, '的': 0.523, '八卦': 1.0, '爵士': 1.0, '就是': 1.0, '男': 1.0, '主角': 1.0, '最後': 1.0, '死': 1.0, '了': 0.699, '脫節': 1.0, '最': 1.0, '系': 1.0, '理論': 1.0, '與': 1.0, '實務': 1.0, '哪個': 1.0, '你問': 1.0, '簡單': 1.0, '多': 1.0, '看': 1.0, '棒球': 1.0, 'PTT': 1.0, '這麼多': 1.0, '肥宅': 0.699, '才': 1.0, '系壘': 1.0, '一堆': 1.0, '胖子': 1.0, '達摩': 1.0, '祖師': 1.0, '傳': 1.0, '那麼': 1.0, '好看': 1.0, '從頭到尾': 1.0, '被動': 1.0, '別人': 1.0, '問他': 1.0, '問題': 1.0, '3D': 1.0, '小': 1.0, '有': 0.699, '人會畫': 1.0, '畫家': 1.0, '當家': 1.0, '對': 1.0, '天龍人': 1.0, '說': 1.0, '宜蘭': 1.0, '4': 1.0, '南部': 1.0, '還': 1.0, '４': 1.0, '東部': 1.0, '他國': 1.0, '事務': 1.0, '機車': 1.0, '推出': 1.0, 'uber': 1.0, '或': 1.0, '計程': 1.0, '怎樣': 1.0, '載到': 1.0, '很': 1.0, '痛苦': 1.0, '台中': 1.0, '龍邦': 1.0, '世貿': 1.0, '跳樓': 1.0, '曾經': 1.0, '當過': 1.0, '全台': 1.0, '第一': 1.0, '高樓': 1.0, '可惜': 1.0, '不到': 1.0, '一年': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file = open(\"./hw1-dataset.txt\", mode='r')\n",
    "    articles = file.readlines()\n",
    "    dataset = preprocessing(articles)\n",
    "    \n",
    "    ### 統計高頻\n",
    "    tf_whole_dataset_table = word_tf_in_dataset(dataset) # 高頻詞(已排序)\n",
    "            \n",
    "    ### 建 tf table\n",
    "    tf_per_article_table = dict()\n",
    "    for article_no in range(len(dataset)):\n",
    "        tf_per_article_table[article_no] = tf(dataset[article_no])\n",
    "    # print(tf_per_article_table) # 取值: tf_per_article_table[article_no][t] = tf(t, d)\n",
    "    \n",
    "    ### 建 idf table\n",
    "    idf_table = idf(tf_per_article_table, len(dataset))\n",
    "    # print(idf_table)\n",
    "    \n",
    "    ### 統計 tf-idf 高\n",
    "    for article_no in range(len(dataset)):\n",
    "        tf_idf_table[article_no] = tf_idf(tf_per_article_table[article_no], idf_table)\n",
    "    \n",
    "    file.close()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['為什麼 聖結石 會被酸而 這群人 不會？\\t質感 劇本 成員 都差很多好嗎 不要拿腎結石來污辱這群人\\n', '為什麼慶祝228會被罵可是慶端午不會？\\t因為屈原不是台灣人，是楚國人。\\n', '有沒有戰神阿瑞斯的八卦?\\t爵士就是阿瑞斯 男主角最後死了\\n']\n"
     ]
    }
   ],
   "source": [
    "dataset = open(\"./hw1-dataset.txt\", mode='r')\n",
    "articles = list()\n",
    "for article in dataset: # article: 一行算一個文章\n",
    "    articles.append(article)\n",
    "    \n",
    "dataset.close()\n",
    "print(articles[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
